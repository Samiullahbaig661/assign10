<h1 class="title-font sm:text-4xl text-3xl mb-4 font-medium text-white">VISION IN OPEN AI</h1><!doctype html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script src="https://cdn.tailwindcss.com"></script>
  <link rel="stylesheet" href="style10.5.css">
</head>
<body>
  

    <header class="text-gray-600 body-font">
        <div class="container mx-auto flex flex-wrap p-5 flex-col md:flex-row items-center">
          <a class="flex title-font font-medium items-center text-gray-900 mb-4 md:mb-0">
           
            <span id="one" class="ml-3 text-xl  " >OPEN AI</span><br><br><br><br>
          </a>
          <nav class="md:ml-auto md:mr-auto flex flex-wrap items-center text-base justify-center">
            <a id="two" class="mr-5 hover:text-gray-900">This Is Last Page</a>
        </div>
    </header>




    <section class="text-gray-400 bg-gray-900 body-font">
        <div class="container mx-auto flex px-5 py-24 items-center justify-center flex-col">
            <h1 id="three" class="title-font sm:text-4xl text-3xl mb-4 font-medium text-white">VISION IN OPEN AI</h1>
            <img class="lg:w-2/6 md:w-3/6 w-5/6 mb-10 object-cover object-center rounded" alt="hero" src="https://i.ytimg.com/vi/hgqhdX3CoW0/maxresdefault.jpg">
          <div class="text-center lg:w-2/3 w-full">
       
            <p id="four" class="leading-relaxed mb-8">GPT-4 with Vision, sometimes referred to as GPT-4V or gpt-4-vision-preview in the API, allows the model to take in images and answer questions about them. Historically, language model systems have been limited by taking in a single input modality, text. For many use cases, this constrained the areas where models like GPT-4 could be used.

                GPT-4 with vision is currently available to all developers who have access to GPT-4 via the gpt-4-vision-preview model and the Chat Completions API which has been updated to support image inputs. Note that the Assistants API does not currently support image inputs. <br>OpenAI, the organization behind the development of the GPT (Generative Pre-trained Transformer) models including GPT-3.5, has a broad vision for artificial intelligence research and development. While I can't provide real-time updates, as of my last update in January 2022, OpenAI's vision revolves around creating artificial general intelligence (AGI) that is capable of understanding and completing a wide range of intellectual tasks at a level comparable to or exceeding that of humans. </p>
            <div class="flex justify-center">
              <!-- <button class="inline-flex text-white bg-indigo-500 border-0 py-2 px-6 focus:outline-none hover:bg-indigo-600 rounded text-lg">Button</button>
              <button class="ml-4 inline-flex text-gray-400 bg-gray-800 border-0 py-2 px-6 focus:outline-none hover:bg-gray-700 hover:text-white rounded text-lg">Button</button>
            </div> -->
          </div>
        </div>
      </section>



      <section class="text-gray-400 bg-gray-900 body-font">
        <div class="container px-5 py-24 mx-auto flex flex-wrap">
          <div class="flex w-full mb-20 flex-wrap">
            <u><h1 id="five" class="sm:text-3xl text-2xl font-medium title-font text-white lg:w-1/3 lg:mb-0 mb-4">TYPES OF VISION</h1></u>
            <p id="six" class="lg:pl-6 lg:w-2/3 mx-auto leading-relaxed text-base">Some key aspects of OpenAI's vision include:</p>
          </div>
          <div class="flex flex-wrap md:-m-2 -m-1">
            <div class="flex flex-wrap w-1/2">
              <div class="md:p-2 p-1 w-1/2">
                <img alt="gallery" class="w-full object-cover h-full object-center block" src="https://eightify.app/summary/artificial-intelligence/openai-s-focus-on-safety-and-ethics-inspires-new-wave-of-ai-development.png">
              </div>
              <div class="md:p-2 p-1 w-1/2">
                <img alt="gallery" class="w-full object-cover h-full object-center block" src="https://www.signitysolutions.com/hubfs/Enhancing%20Web%20Accessibility%20with%20OpenAI.jpg#keepProtocol">
              </div>
              <div class="md:p-2 p-1 w-full">
                <img alt="gallery" class="w-full h-full object-cover object-center block" src="https://www.yugabyte.com/wp-content/uploads/2023/10/YugabyteDB_Blog-Build-Scalable-Generative-AI-Applications-with-Azure-OpenAI-and-YugabyteDB_480x320.jpg">
              </div>
            </div>
            <div class="flex flex-wrap w-1/2">
              <div class="md:p-2 p-1 w-full">
                <img alt="gallery" class="w-full h-full object-cover object-center block" src="https://i.ytimg.com/vi/LjJWCgJ1A7U/maxresdefault.jpg">
              </div>
              <div class="md:p-2 p-1 w-1/2">
                <img alt="gallery" class="w-full object-cover h-full object-center block" src="https://cdn.analyticsvidhya.com/wp-content/uploads/2023/10/Screenshot_2023-09-27_at_6.21.09_PM_IQIR2W6.png">
              </div>
              <div class="md:p-2 p-1 w-1/2">
                <img alt="gallery" class="w-full object-cover h-full object-center block" src="https://media.licdn.com/dms/image/D4E12AQGR6XKp0ELyWw/article-cover_image-shrink_720_1280/0/1678666421151?e=2147483647&v=beta&t=5f9_efJV6A3jm4ahsHtaqH53K3WWxA9EDXb2Y6neoM0">
              </div>
            </div>
          </div>
        </div>
      </section>

</body>
</html>